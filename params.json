{"name":"Customer Churn ML","tagline":"","body":"# Customer Churn analysis model for Software-as-a-Service business models\r\n\r\n### Michael Ransby, Brandon Beck\r\n\r\n## Problem Definition\r\nBusiness analysts often find themselves trying to optimize for the customer experience, with a \"red flag\" being the churn rate of customers - they've been sucked in but aren't convinced the value provided exceeds the value attained through continued payments for a given service. \r\nThis project aims to serve as a proof of concept for a given business to analyze various customer metrics - and ensure they are informed by those that correlate with the real world. \r\n\r\n### Citations\r\nThe first citation would be a conversation between Michael and a colleague/mentor. \r\n\r\nThe conversation piqued an interest in machine learning and data analysis for business analytic pipelines. This project serves as a proof of concept to build a pipeline around, such that Michael can continue to provide data-driven business analytics.\r\n\r\nhttps://medium.com/@paldesk/what-is-customer-churn-how-to-reduce-it-402460e5b569\r\n\r\nhttps://www.profitwell.com/recur/all/involuntary-delinquent-churn-failed-payments-recovery\r\n\r\n## Data Collection\r\nLooking through the various datasets available, we found many datasets on Kaggle. Many of these have a high number of observations, and features - allowing for demonstration of an explained variance process. \r\n\r\nThe high dimensionality of the dataset will allow us to identify key attributes of customers that have an influence or indication about their likelihood to churn. \r\n\r\nKaggle had the entire dataset available for download as a CSV, which is a straightforward filetype to handle.\r\n\r\n## Data Preprocessing\r\nThe dataset can be handled easily using pandas. The dataset contains a few datatypes so something like NumPy cannot be used. Pandas is easily able to clean the data of points that contain empty or invalid cells. When something empty or invalid is found that entire data point is removed as opposed to interpolating the data by for example taking the mean of that feature (for numerical features). Something like taking the mean would only serve to bias the incomplete user data towards the others, so we avoid this.\r\n\r\n## Methods\r\n### Encoding\r\nUsing the `pandas` library, we are simply able to assign a datatype (category, float, etc) to import our data from the CSV format given. We now have a data frame, which is easily usable for our purposes.\r\n\r\nNominal variables have been \"dummied\", such that each unique value in each category becomes a boolean.\r\n\r\nOrdinal variables (an example would be credit score) are to be encoded as integers, remaining in place in a single column.\r\n\r\n### Preliminary Data Check\r\nChecking the balance of the classes (Churned/Not Churned), we see a distribution of 26.5% Not and 73.5% churned. To overcome this we used the SMOTE method from the library `imblearn` to balance the classes for training and testing. It is not mentioned in the notebook, however, this boosted accuracy across all models tested by roughly 5%.\r\n\r\n### Feature Importance\r\nWe used an implementation of the random forest classifier from the previous homework to find the importance of each feature in the dataset. By fitting the data we can make a plot of the feature importance attribute to identify the relevant features in the dataset. \r\n\r\nInterestingly, the feature making a customer least likely to churn is their tenure (the number of months they have been retained as a customer). This is consistent with a customer being on a \"month to month\" contract, which most positively indicates the churn of a given customer.\r\n\r\n![Feature_Importances](images/feature_importance.png)\r\n\r\n### SVD 2D\r\n\r\n![SVD_2D](images/SVD_2D.png)\r\n\r\n### SVD 3D\r\n\r\n![UPDATE THIS](images/SVD_3D.png)\r\n\r\n### Model Selection \r\nIn the [Jupyter Notebook](https://github.gatech.edu/mransby3/machine-learning-project/blob/master/churn-notebook.ipynb), we perform a search across many parameters across DecisionTree, AdaBoost, RandomForest, Logistic Regression, and a Tensorflow Sequential Neural Network through the use of the GridSearchCV tool built into scikit-learn. See the notebook for accuracy per model and their corresponding confusion matrices \r\n\r\n## Results\r\n\r\nThe Random Forest Classifier was shown to have the highest accuracy (83%) across our testing, with the following confusion matrix.\r\n\r\n![Random-Forest-Confusion-Matrix](images/Random_Forest_CM.png)\r\n\r\nThe feature importance plot outlined in our methods showed that a customer’s loyalty to the provider is the highest correlating feature in the set. After loyalty, the existence of a month-to-month contract weighs heavily in a customer’s likelihood to churn. This is followed closely by both numerical values surrounding the cost, with total cost weighing a lot more. After these, the top features generally surround the quality of the product and ease of service provided by the company. Features such as gender, age, marriage status, and having dependents weighed very low in the indication of churn.\r\n\r\n## Discussion\r\n**Put graphs in here and have a little one liner about each of them. Talk about what we did to the data and what the results are telling us**\r\n\r\n## Conclusion\r\nAs a result of this study, we can affirm that certain attributes like **INSERT BEST FEATURES HERE** can be used with high accuracy to determine which customers are likely to discontinue your service or switch to another provider. **ANYTHING ELSE FROM RESULTS THAT COULD BE CONSIDERED A RECOMMENDATION WE MAKE FROM THIS STUDY**\r\nThis information is critical for companies that struggle with retention or want to target certain groups to gain traction with their demographic. Using targeted deals and pricing, this model can show a company that users need a nudge to remain loyal to their brand. In this instance, phone companies can analyze the prices to set and deals to offer when comparing to their rival providers.\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}